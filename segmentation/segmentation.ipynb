{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # 忽略警告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def transfer_label(label):\n",
    "    width, height, channel = label.shape\n",
    "    res = np.zeros_like(label)\n",
    "    channel = label[:, :, 0]\n",
    "    for row_idx in range(height):\n",
    "        for col_idx in range(width):\n",
    "            color = channel[row_idx, col_idx]\n",
    "            channel_map = {0 : 0, 2 : 1, 3 : 2}\n",
    "            res[row_idx, col_idx, channel_map[channel[row_idx, col_idx]]] = 1\n",
    "    return res\n",
    "            \n",
    "\n",
    "def load_image(type):\n",
    "    root = f\"./road-segmenttation-6/{type}\"\n",
    "    mask_image_list = []\n",
    "    origin_image_list = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(root):\n",
    "        for file in files:\n",
    "            mask_file_path = os.path.join(root, file)\n",
    "            if mask_file_path[-3:] == \"png\":\n",
    "                mask_frame = cv2.imread(mask_file_path)\n",
    "                mask_frame_rgb = mask_frame[:, :, ::-1].copy()\n",
    "                mask_frame_label = transfer_label(mask_frame_rgb)\n",
    "                mask_image_list.append(mask_frame_label)\n",
    "                \n",
    "                origin_file_path = mask_file_path[:-9] + \".jpg\"\n",
    "                origin_frame = cv2.imread(origin_file_path)\n",
    "                origin_frame_rgb = origin_frame[:, :, ::-1].copy()\n",
    "                origin_image_list.append(origin_frame_rgb)\n",
    "                \n",
    "    mask_image_np = np.vstack(mask_image_list).reshape((-1, 640, 640, 3))\n",
    "    origin_image_np = np.vstack(origin_image_list).reshape((-1, 640, 640, 3))\n",
    "    \n",
    "    images, labels = shuffle(origin_image_np, mask_image_np)\n",
    "    print(f'images.shape: {images.shape}')\n",
    "    print(f'labels.shape: {labels.shape}')\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: (150, 640, 640, 3)\n",
      "labels.shape: (150, 640, 640, 3)\n",
      "images.shape: (8, 640, 640, 3)\n",
      "labels.shape: (8, 640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = load_image('train')\n",
    "test_images, test_labels = load_image('test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary items from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, UpSampling2D\n",
    "from keras.layers import Conv2DTranspose, Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, pool_size):\n",
    "    # 创建网络模型\n",
    "    model = Sequential()\n",
    "    # 对输入层进行归一化处理\n",
    "    print(f\"input shape = {input_shape}\")\n",
    "    model.add(BatchNormalization(input_shape=input_shape))\n",
    "\n",
    "    # 卷积层1，名为Conv1\n",
    "    model.add(Conv2D(8, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv1'))\n",
    "\n",
    "    # 卷积层2\n",
    "    model.add(Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv2'))\n",
    "\n",
    "    # 最大化层\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    # 卷积层3\n",
    "    model.add(Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv3'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 卷积层4\n",
    "    model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv4'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 卷积层5\n",
    "    model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv5'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 最大化层2\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    # 卷积层6\n",
    "    model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv6'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 卷积层7\n",
    "    model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv7'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 最大化层3\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    # 上采样层1\n",
    "    model.add(UpSampling2D(size=pool_size))\n",
    "\n",
    "    # 反卷积层 1\n",
    "    model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv1'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 反卷积层 2\n",
    "    model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv2'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 上采样层 2\n",
    "    model.add(UpSampling2D(size=pool_size))\n",
    "\n",
    "    # 反卷积层 3\n",
    "    model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv3'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 反卷积层 4\n",
    "    model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv4'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 反卷积层 5\n",
    "    model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv5'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 上采样层 3\n",
    "    model.add(UpSampling2D(size=pool_size))\n",
    "\n",
    "    # 反卷积层 6\n",
    "    model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv6'))\n",
    "\n",
    "    # 输出层\n",
    "    model.add(Conv2DTranspose(3, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Final'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: (150, 640, 640, 3)\n",
      "labels.shape: (150, 640, 640, 3)\n",
      "images.shape: (14, 640, 640, 3)\n",
      "labels.shape: (14, 640, 640, 3)\n",
      "input shape = (640, 640, 3)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_6 (Bat  (None, 640, 640, 3)       12        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " Conv1 (Conv2D)              (None, 638, 638, 8)       224       \n",
      "                                                                 \n",
      " Conv2 (Conv2D)              (None, 636, 636, 16)      1168      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 318, 318, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " Conv3 (Conv2D)              (None, 316, 316, 16)      2320      \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 316, 316, 16)      0         \n",
      "                                                                 \n",
      " Conv4 (Conv2D)              (None, 314, 314, 32)      4640      \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 314, 314, 32)      0         \n",
      "                                                                 \n",
      " Conv5 (Conv2D)              (None, 312, 312, 32)      9248      \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 312, 312, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 156, 156, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " Conv6 (Conv2D)              (None, 154, 154, 64)      18496     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 154, 154, 64)      0         \n",
      "                                                                 \n",
      " Conv7 (Conv2D)              (None, 152, 152, 64)      36928     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 152, 152, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 76, 76, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " up_sampling2d_6 (UpSamplin  (None, 152, 152, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " Deconv1 (Conv2DTranspose)   (None, 154, 154, 64)      36928     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 154, 154, 64)      0         \n",
      "                                                                 \n",
      " Deconv2 (Conv2DTranspose)   (None, 156, 156, 64)      36928     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 156, 156, 64)      0         \n",
      "                                                                 \n",
      " up_sampling2d_7 (UpSamplin  (None, 312, 312, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " Deconv3 (Conv2DTranspose)   (None, 314, 314, 32)      18464     \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 314, 314, 32)      0         \n",
      "                                                                 \n",
      " Deconv4 (Conv2DTranspose)   (None, 316, 316, 32)      9248      \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 316, 316, 32)      0         \n",
      "                                                                 \n",
      " Deconv5 (Conv2DTranspose)   (None, 318, 318, 16)      4624      \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 318, 318, 16)      0         \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSamplin  (None, 636, 636, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " Deconv6 (Conv2DTranspose)   (None, 638, 638, 16)      2320      \n",
      "                                                                 \n",
      " Final (Conv2DTranspose)     (None, 640, 640, 3)       435       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181983 (710.87 KB)\n",
      "Trainable params: 181977 (710.85 KB)\n",
      "Non-trainable params: 6 (24.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "pool_size = (2, 2)\n",
    "images, labels = load_image('train')\n",
    "val_images, val_labels = load_image('valid')\n",
    "input_shape = images.shape[1:]\n",
    "model = create_model(input_shape, pool_size)\n",
    "\n",
    "datagen = ImageDataGenerator(channel_shift_range=0.2)\n",
    "datagen.fit(images)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "# 可视化模型\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 71s 13s/step - loss: 0.2918 - val_loss: 0.5707\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 66s 13s/step - loss: 0.2210 - val_loss: 0.2022\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 63s 12s/step - loss: 0.2043 - val_loss: 0.2051\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 67s 13s/step - loss: 0.1943 - val_loss: 0.2051\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 69s 14s/step - loss: 0.1887 - val_loss: 0.2009\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 65s 13s/step - loss: 0.1808 - val_loss: 0.1928\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 64s 13s/step - loss: 0.1626 - val_loss: 0.1899\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 70s 14s/step - loss: 0.1353 - val_loss: 0.1799\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 66s 13s/step - loss: 0.1043 - val_loss: 0.1671\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 69s 14s/step - loss: 0.0813 - val_loss: 0.1973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2e977f040>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(images, labels, batch_size=30), steps_per_epoch=len(images)/30,\n",
    "epochs=epochs, verbose=1, validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_try.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def create_video(path, model):\n",
    "    vs = cv2.VideoCapture(path)\n",
    "    while True:\n",
    "        (grabbed, frame_source) = vs.read()\n",
    "        if not grabbed: break\n",
    "        \n",
    "        height, width = frame_source.shape[:2]\n",
    "        \n",
    "        frame = cv2.resize(frame_source ,(640, 640))\n",
    "        \n",
    "        # 为frame_input添加一个维度\n",
    "        frame_input = frame[None, :, :, :]\n",
    "        prediction = model.predict(frame_input)\n",
    "        prediction = prediction[0] * 255\n",
    "        \n",
    "        blank = cv2.resize(prediction, (width, height))\n",
    "        output = cv2.addWeighted(frame_source, 0.3, blank, 0.7, 0, dtype = cv2.CV_32F)\n",
    "        \n",
    "        # 清空绘图空间\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # 显示处理结果\n",
    "        _, jpg = cv2.imencode('.jpg', output)\n",
    "        display(jpg)\n",
    "\n",
    "        #按键盘中的q键退出检测\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):   break\n",
    "        \n",
    "    # 释放资源\n",
    "    print(\"[INFO] cleaning up...\")\n",
    "    vs.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m create_video(\u001b[39m'\u001b[39m\u001b[39m../pose/test.mp4\u001b[39m\u001b[39m'\u001b[39m, model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "create_video('../pose/test.mp4', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a66a995458df657f2d1dd1215d3fd21014c75477484430839b4fdbbf23622b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
