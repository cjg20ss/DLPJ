{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # 忽略警告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def transfer_label(label):\n",
    "    width, height, channel = label.shape\n",
    "    res = np.zeros_like(label)\n",
    "    channel = label[:, :, 0]\n",
    "    for row_idx in range(height):\n",
    "        for col_idx in range(width):\n",
    "            color = channel[row_idx, col_idx]\n",
    "            channel_map = {0 : 0, 2 : 1, 3 : 2}\n",
    "            res[row_idx, col_idx, channel_map[channel[row_idx, col_idx]]] = 1\n",
    "    return res\n",
    "            \n",
    "\n",
    "def load_image(type):\n",
    "    root = f\"./road-segmenttation-6/{type}\"\n",
    "    mask_image_list = []\n",
    "    origin_image_list = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(root):\n",
    "        for file in files:\n",
    "            mask_file_path = os.path.join(root, file)\n",
    "            if mask_file_path[-3:] == \"png\":\n",
    "                mask_frame = cv2.imread(mask_file_path)\n",
    "                mask_frame_rgb = mask_frame[:, :, ::-1].copy()\n",
    "                mask_frame_label = transfer_label(mask_frame_rgb)\n",
    "                mask_image_list.append(mask_frame_label)\n",
    "                \n",
    "                origin_file_path = mask_file_path[:-9] + \".jpg\"\n",
    "                origin_frame = cv2.imread(origin_file_path)\n",
    "                origin_frame_rgb = origin_frame[:, :, ::-1].copy()\n",
    "                origin_image_list.append(origin_frame_rgb)\n",
    "                \n",
    "    mask_image_np = np.vstack(mask_image_list).reshape((-1, 640, 640, 3))\n",
    "    origin_image_np = np.vstack(origin_image_list).reshape((-1, 640, 640, 3))\n",
    "    \n",
    "    images, labels = shuffle(origin_image_np, mask_image_np)\n",
    "    print(f'images.shape: {images.shape}')\n",
    "    print(f'labels.shape: {labels.shape}')\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: (150, 640, 640, 3)\n",
      "labels.shape: (150, 640, 640, 3)\n",
      "images.shape: (8, 640, 640, 3)\n",
      "labels.shape: (8, 640, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = load_image('train')\n",
    "test_images, test_labels = load_image('test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary items from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, UpSampling2D\n",
    "from keras.layers import Conv2DTranspose, Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, pool_size):\n",
    "    # 创建网络模型\n",
    "    model = Sequential()\n",
    "    # 对输入层进行归一化处理\n",
    "    print(f\"input shape = {input_shape}\")\n",
    "    model.add(BatchNormalization(input_shape=input_shape))\n",
    "\n",
    "    # 卷积层1，名为Conv1\n",
    "    model.add(Conv2D(8, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv1'))\n",
    "\n",
    "    # 卷积层2\n",
    "    model.add(Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv2'))\n",
    "\n",
    "    # 最大化层\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    # 卷积层3\n",
    "    model.add(Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv3'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 卷积层4\n",
    "    model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv4'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 卷积层5\n",
    "    model.add(Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv5'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 最大化层2\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    # 卷积层6\n",
    "    model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv6'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 卷积层7\n",
    "    model.add(Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Conv7'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 最大化层3\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "    # 上采样层1\n",
    "    model.add(UpSampling2D(size=pool_size))\n",
    "\n",
    "    # 反卷积层 1\n",
    "    model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv1'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 反卷积层 2\n",
    "    model.add(Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv2'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 上采样层 2\n",
    "    model.add(UpSampling2D(size=pool_size))\n",
    "\n",
    "    # 反卷积层 3\n",
    "    model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv3'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 反卷积层 4\n",
    "    model.add(Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv4'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 反卷积层 5\n",
    "    model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv5'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 上采样层 3\n",
    "    model.add(UpSampling2D(size=pool_size))\n",
    "\n",
    "    # 反卷积层 6\n",
    "    model.add(Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Deconv6'))\n",
    "\n",
    "    # 输出层\n",
    "    model.add(Conv2DTranspose(3, (3, 3), padding='valid', strides=(1,1), activation = 'relu', name = 'Final'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: (150, 640, 640, 3)\n",
      "labels.shape: (150, 640, 640, 3)\n",
      "images.shape: (14, 640, 640, 3)\n",
      "labels.shape: (14, 640, 640, 3)\n",
      "input shape = (640, 640, 3)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_6 (Bat  (None, 640, 640, 3)       12        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " Conv1 (Conv2D)              (None, 638, 638, 8)       224       \n",
      "                                                                 \n",
      " Conv2 (Conv2D)              (None, 636, 636, 16)      1168      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 318, 318, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " Conv3 (Conv2D)              (None, 316, 316, 16)      2320      \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 316, 316, 16)      0         \n",
      "                                                                 \n",
      " Conv4 (Conv2D)              (None, 314, 314, 32)      4640      \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 314, 314, 32)      0         \n",
      "                                                                 \n",
      " Conv5 (Conv2D)              (None, 312, 312, 32)      9248      \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 312, 312, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 156, 156, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " Conv6 (Conv2D)              (None, 154, 154, 64)      18496     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 154, 154, 64)      0         \n",
      "                                                                 \n",
      " Conv7 (Conv2D)              (None, 152, 152, 64)      36928     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 152, 152, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 76, 76, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " up_sampling2d_6 (UpSamplin  (None, 152, 152, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " Deconv1 (Conv2DTranspose)   (None, 154, 154, 64)      36928     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 154, 154, 64)      0         \n",
      "                                                                 \n",
      " Deconv2 (Conv2DTranspose)   (None, 156, 156, 64)      36928     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 156, 156, 64)      0         \n",
      "                                                                 \n",
      " up_sampling2d_7 (UpSamplin  (None, 312, 312, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " Deconv3 (Conv2DTranspose)   (None, 314, 314, 32)      18464     \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 314, 314, 32)      0         \n",
      "                                                                 \n",
      " Deconv4 (Conv2DTranspose)   (None, 316, 316, 32)      9248      \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 316, 316, 32)      0         \n",
      "                                                                 \n",
      " Deconv5 (Conv2DTranspose)   (None, 318, 318, 16)      4624      \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 318, 318, 16)      0         \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSamplin  (None, 636, 636, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " Deconv6 (Conv2DTranspose)   (None, 638, 638, 16)      2320      \n",
      "                                                                 \n",
      " Final (Conv2DTranspose)     (None, 640, 640, 3)       435       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181983 (710.87 KB)\n",
      "Trainable params: 181977 (710.85 KB)\n",
      "Non-trainable params: 6 (24.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "pool_size = (2, 2)\n",
    "images, labels = load_image('train')\n",
    "val_images, val_labels = load_image('valid')\n",
    "input_shape = images.shape[1:]\n",
    "model = create_model(input_shape, pool_size)\n",
    "\n",
    "datagen = ImageDataGenerator(channel_shift_range=0.2)\n",
    "datagen.fit(images)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "# 可视化模型\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 71s 13s/step - loss: 0.2918 - val_loss: 0.5707\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 66s 13s/step - loss: 0.2210 - val_loss: 0.2022\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 63s 12s/step - loss: 0.2043 - val_loss: 0.2051\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 67s 13s/step - loss: 0.1943 - val_loss: 0.2051\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 69s 14s/step - loss: 0.1887 - val_loss: 0.2009\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 65s 13s/step - loss: 0.1808 - val_loss: 0.1928\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 64s 13s/step - loss: 0.1626 - val_loss: 0.1899\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 70s 14s/step - loss: 0.1353 - val_loss: 0.1799\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 66s 13s/step - loss: 0.1043 - val_loss: 0.1671\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 69s 14s/step - loss: 0.0813 - val_loss: 0.1973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2e977f040>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(images, labels, batch_size=30), steps_per_epoch=len(images)/30,\n",
    "epochs=epochs, verbose=1, validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_try.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def create_video(path, model):\n",
    "    vs = cv2.VideoCapture(path)\n",
    "    while True:\n",
    "        (grabbed, frame_source) = vs.read()\n",
    "        if not grabbed: break\n",
    "        \n",
    "        height, width = frame_source.shape[:2]\n",
    "        \n",
    "        frame = cv2.resize(frame_source ,(640, 640))\n",
    "        \n",
    "        # 为frame_input添加一个维度\n",
    "        frame_input = frame[None, :, :, :]\n",
    "        prediction = model.predict(frame_input)\n",
    "        prediction = prediction[0] * 255\n",
    "        \n",
    "        blank = cv2.resize(prediction, (width, height))\n",
    "        output = cv2.addWeighted(frame_source, 0.3, blank, 0.7, 0, dtype = cv2.CV_32F)\n",
    "        \n",
    "        # 清空绘图空间\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # 显示处理结果\n",
    "        _, jpg = cv2.imencode('.jpg', output)\n",
    "        display(jpg)\n",
    "\n",
    "        #按键盘中的q键退出检测\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):   break\n",
    "        \n",
    "    # 释放资源\n",
    "    print(\"[INFO] cleaning up...\")\n",
    "    vs.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255, 216, 255, ..., 207, 255, 217], dtype=uint8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/caishengyan/Desktop/Fudan/大三下/深度学习/DLPJ/segmentation/segmentation.ipynb 单元格 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/caishengyan/Desktop/Fudan/%E5%A4%A7%E4%B8%89%E4%B8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/DLPJ/segmentation/segmentation.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m create_video(\u001b[39m'\u001b[39;49m\u001b[39m../pose/test.mp4\u001b[39;49m\u001b[39m'\u001b[39;49m, model)\n",
      "\u001b[1;32m/Users/caishengyan/Desktop/Fudan/大三下/深度学习/DLPJ/segmentation/segmentation.ipynb 单元格 15\u001b[0m in \u001b[0;36mcreate_video\u001b[0;34m(path, model)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caishengyan/Desktop/Fudan/%E5%A4%A7%E4%B8%89%E4%B8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/DLPJ/segmentation/segmentation.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# 为frame_input添加一个维度\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caishengyan/Desktop/Fudan/%E5%A4%A7%E4%B8%89%E4%B8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/DLPJ/segmentation/segmentation.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m frame_input \u001b[39m=\u001b[39m frame[\u001b[39mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/caishengyan/Desktop/Fudan/%E5%A4%A7%E4%B8%89%E4%B8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/DLPJ/segmentation/segmentation.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(frame_input)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caishengyan/Desktop/Fudan/%E5%A4%A7%E4%B8%89%E4%B8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/DLPJ/segmentation/segmentation.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m prediction \u001b[39m=\u001b[39m prediction[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caishengyan/Desktop/Fudan/%E5%A4%A7%E4%B8%89%E4%B8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/DLPJ/segmentation/segmentation.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m blank \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(prediction, (width, height))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/my_env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/my_env/lib/python3.8/site-packages/keras/src/engine/training.py:2554\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2552\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[1;32m   2553\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 2554\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[1;32m   2555\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   2556\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/my_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/my_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/my_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    865\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/my_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/my_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/my_env/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/my_env/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/my_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_video('../pose/test.mp4', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
